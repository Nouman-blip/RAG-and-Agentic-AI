{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install \"ibm-watsonx-ai==1.0.8\" --user\n",
    "!pip install \"langchain==0.2.11\" --user\n",
    "!pip install \"langchain-ibm==0.1.7\" --user\n",
    "!pip install \"langchain-core==0.2.43\" --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ibm import WatsonxLLM\n",
    "\n",
    "granite_llm = WatsonxLLM(\n",
    "    model_id=\"ibm/granite-3-2-8b-instruct\",\n",
    "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "    project_id=\"skills-network\",\n",
    "    params={\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"temperature\": 0.5,\n",
    "        \"top_p\": 0.2\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_model(prompt_txt, params=None):\n",
    "    model_id = \"ibm/granite-3-2-8b-instruct\"\n",
    "    default_params = {\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"temperature\": 0.5,\n",
    "        \"top_p\": 0.2\n",
    "    }\n",
    "    if params:\n",
    "        default_params.update(params)\n",
    "\n",
    "    granite_llm = WatsonxLLM(\n",
    "        model_id=model_id,\n",
    "        url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "        project_id=\"skills-network\",\n",
    "        params=default_params\n",
    "    )\n",
    "\n",
    "    response = granite_llm.invoke(prompt_txt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "\n",
    "GenParams().get_example_values()\n",
    "\n",
    "parameters = {\n",
    "    GenParams.MAX_NEW_TOKENS: 256,\n",
    "    GenParams.TEMPERATURE: 0.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"min_new_tokens\": 10,\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_p\": 0.2,\n",
    "    \"top_k\": 1\n",
    "}\n",
    "prompt = \"The wind is\"\n",
    "\n",
    "response = llm_model(prompt, params)\n",
    "print(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Classify the following statement as true or false:\n",
    "'The Eiffel Tower is located in Berlin.'\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "response = llm_model(prompt, params)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"max_new_tokens\": 20,\n",
    "    \"temperature\": 0.1,\n",
    "}\n",
    "prompt = \"\"\"Here is an example of translating a sentence from English to French:\n",
    "\n",
    "English: \"How is the weather today?\"\n",
    "French: \"Comment est le temps aujourd'hui?\"\n",
    "\n",
    "Now translate:\n",
    "English: \"Where is the nearest supermarket?\"\n",
    "\"\"\"\n",
    "\n",
    "response = llm_model(prompt, params)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"max_new_tokens\": 10}\n",
    "prompt = \"\"\"Here are few examples of classifying emotions:\n",
    "\n",
    "Statement: I just won my first marathon!\n",
    "Emotion: Joy\n",
    "\n",
    "Statement: I canâ€™t believe I lost my keys again.\n",
    "Emotion: Frustration\n",
    "\n",
    "Statement: My best friend is moving.\n",
    "Emotion: Sadness\n",
    "\n",
    "Now classify:\n",
    "Statement: That movie was so scary I had to cover my eyes.\n",
    "\"\"\"\n",
    "\n",
    "print(llm_model(prompt, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template = \"\"\"Tell me a {adjective} joke about {content}.\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "def format_prompt(vars):\n",
    "    return prompt.format(**vars)\n",
    "\n",
    "chain = (\n",
    "    RunnableLambda(format_prompt)\n",
    "    | granite_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(chain.invoke({\"adjective\":\"funny\",\"content\":\"chickens\"}))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
