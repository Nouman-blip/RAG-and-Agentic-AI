{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of Generative AI & LangChain\n",
    "\n",
    "This notebook introduces:\n",
    "- IBM Watsonx LLM\n",
    "- Prompting techniques\n",
    "- LangChain basics\n",
    "- LCEL chaining\n",
    "\n",
    "Estimated time: 10 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Packages\n",
    "Installs Watsonx, LangChain, and core dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install \"ibm-watsonx-ai==1.0.8\" --user\n",
    "!pip install \"langchain==0.2.11\" --user\n",
    "!pip install \"langchain-ibm==0.1.7\" --user\n",
    "!pip install \"langchain-core==0.2.43\" --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suppress Warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Watsonx Granite Model\n",
    "Creates a connection to IBM Granite LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ibm import WatsonxLLM\n",
    "\n",
    "granite_llm = WatsonxLLM(\n",
    "    model_id=\"ibm/granite-3-2-8b-instruct\",\n",
    "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "    project_id=\"skills-network\",\n",
    "    params={\"max_new_tokens\":256,\"temperature\":0.5,\"top_p\":0.2}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function\n",
    "Reusable function to invoke the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_model(prompt_txt, params=None):\n",
    "    default={\"max_new_tokens\":256,\"temperature\":0.5,\"top_p\":0.2}\n",
    "    if params: default.update(params)\n",
    "    llm=WatsonxLLM(model_id=\"ibm/granite-3-2-8b-instruct\",url=\"https://us-south.ml.cloud.ibm.com\",project_id=\"skills-network\",params=default)\n",
    "    return llm.invoke(prompt_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation Parameters\n",
    "Shows IBM GenText parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "GenParams().get_example_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\"max_new_tokens\":128}\n",
    "print(llm_model(\"The wind is\",params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"Classify true or false:\n",
    "The Eiffel Tower is located in Berlin.\n",
    "\"\"\"\n",
    "print(llm_model(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Shot Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"English: Hello\n",
    "French: Bonjour\n",
    "Translate:\n",
    "English: Where is the station?\n",
    "\"\"\"\n",
    "print(llm_model(prompt,{\"temperature\":0.1}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"I won the race! -> Joy\n",
    "I lost my wallet -> Sadness\n",
    "That horror movie scared me ->\n",
    "\"\"\"\n",
    "print(llm_model(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain LCEL Example\n",
    "PromptTemplate + RunnableLambda + OutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template=\"Tell me a {adj} joke about {thing}.\"\n",
    "prompt=PromptTemplate.from_template(template)\n",
    "\n",
    "def fmt(v): return prompt.format(**v)\n",
    "\n",
    "chain=(RunnableLambda(fmt)|granite_llm|StrOutputParser())\n",
    "\n",
    "print(chain.invoke({\"adj\":\"funny\",\"thing\":\"cats\"}))"
   ]
  }
 ],
 \"metadata\": {},
 \"nbformat\": 4,
 \"nbformat_minor\": 5
}
